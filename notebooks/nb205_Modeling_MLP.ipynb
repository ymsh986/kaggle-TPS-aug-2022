{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPS-Aug-2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    NB = '205'\n",
    "    dataset_NB = '104'\n",
    "\n",
    "    raw_data_dir = '../data/raw/'\n",
    "    processed_data_dir = '../data/processed/'\n",
    "    interim_dir = '../data/interim/'\n",
    "    submission_dir = '../data/submission/'\n",
    "\n",
    "    random_seed = 42\n",
    "    n_folds = 5\n",
    "\n",
    "    row_id = 'id'\n",
    "    target = 'failure'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libralies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style='white', context='notebook', palette='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "plotly_template = dict(\n",
    "    layout=go.Layout(\n",
    "        template='plotly_dark',\n",
    "        font=dict(\n",
    "            family=\"Franklin Gothic\",\n",
    "            size=12\n",
    "        ),\n",
    "        height=500,\n",
    "        width=1000,\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "color_palette = {\n",
    "    'Bin': ['#016CC9','#E876A3'],\n",
    "    'Cat5': ['#E876A3', '#E0A224', '#63B70D', '#6BCFF6', '#13399E'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, train_test_split, cross_validate, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, confusion_matrix, roc_auc_score, roc_curve, auc\n",
    "from scipy.stats import mode\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26570, 29)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_pickle(Config.processed_data_dir + f'nb{Config.dataset_NB}_train.pkl', compression='zip')\n",
    "df_test = pd.read_pickle(Config.processed_data_dir + f'nb{Config.dataset_NB}_test.pkl', compression='zip')\n",
    "\n",
    "submission = pd.read_csv(Config.raw_data_dir + 'sample_submission.csv')\n",
    "\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loading',\n",
       " 'attribute_2',\n",
       " 'attribute_3',\n",
       " 'measurement_0',\n",
       " 'measurement_1',\n",
       " 'measurement_2',\n",
       " 'measurement_3',\n",
       " 'measurement_4',\n",
       " 'measurement_5',\n",
       " 'measurement_6',\n",
       " 'measurement_7',\n",
       " 'measurement_8',\n",
       " 'measurement_9',\n",
       " 'measurement_10',\n",
       " 'measurement_11',\n",
       " 'measurement_12',\n",
       " 'measurement_13',\n",
       " 'measurement_14',\n",
       " 'measurement_15',\n",
       " 'measurement_16',\n",
       " 'measurement_17',\n",
       " 'attribute_0_material_5',\n",
       " 'attribute_0_material_7',\n",
       " 'attribute_1_material_5',\n",
       " 'attribute_1_material_6',\n",
       " 'attribute_1_material_7',\n",
       " 'attribute_1_material_8']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_list = [col for col in df_train.columns if col not in [Config.row_id, Config.target]]\n",
    "feature_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation data Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20775, 27)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = df_test[feature_list]\n",
    "\n",
    "'''\n",
    "for c in TARGET_ENCODING_CATEGORY:\n",
    "    data_tmp = pd.DataFrame({c: df_train[c], 'target': df_train[TARGET]})\n",
    "    target_mean = data_tmp.groupby(c)['target'].mean()\n",
    "    X_test.loc[:, c] = X_test[c].map(target_mean)\n",
    "'''\n",
    "\n",
    "X_test = (X_test.values).astype(np.float32)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Layer Perceptron\n",
    "- 隠れ層3層のMLP\n",
    "- kernel_initializerにHeの初期化を採用\n",
    "- Batch Normalizationを採用\n",
    "- 活性化関数にReLUを採用\n",
    "- Optimizerを採用（SGD、Adamなど。）\n",
    "- Dropoutを採用\n",
    "  - DropoutとBatchNormalizationを同時に使うと学習がうまくできない場合がある。\n",
    "  - その場合、Dropoutを外す\n",
    "- モデルの順序は、BatchNormalization、活性化関数、Dropoutであることに注意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return tf.keras.backend.sqrt(tf.keras.backend.mean(tf.keras.backend.square(y_pred - y_true)))\n",
    "\n",
    "def setup_model():\n",
    "    activation = 'relu'\n",
    "    kernel_initializer = 'he_normal'\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    '''\n",
    "    model.add(Dense(96, kernel_initializer=kernel_initializer))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(64, kernel_initializer=kernel_initializer))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dropout(0.25))\n",
    "    '''\n",
    "    model.add(Dense(32, kernel_initializer=kernel_initializer))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation))\n",
    "    # model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(16, kernel_initializer=kernel_initializer))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation))\n",
    "    # model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(8, kernel_initializer=kernel_initializer))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation))\n",
    "    # model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(1, activation='softmax'))\n",
    "\n",
    "    optimizer = optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, amsgrad=True)\n",
    "    # optimizer = optimizers.SGD(learning_rate=0.001)\n",
    "\n",
    "    # model.compile(optimizer=optimizer, loss=root_mean_squared_error, metrics=[root_mean_squared_error])\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])\n",
    "\n",
    "    return model\n",
    "\n",
    "def setup_callbacks():\n",
    "    es = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "    lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.7, patience=5, verbose=1)\n",
    "    callbacks = [es, lr]\n",
    "\n",
    "    return callbacks\n",
    "\n",
    "\n",
    "mlp_param = {\n",
    "    'epochs': 300,\n",
    "    'batch_size': 100,\n",
    "    'verbose': 1,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Validation with TargetEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "213/213 [==============================] - 2s 4ms/step - loss: 0.5300 - auc_13: 0.5000 - val_loss: 0.4975 - val_auc_13: 0.5000 - lr: 0.0100\n",
      "Epoch 2/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.5160 - auc_13: 0.5000 - val_loss: 0.4978 - val_auc_13: 0.5000 - lr: 0.0100\n",
      "Epoch 3/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.5145 - auc_13: 0.5000 - val_loss: 0.4955 - val_auc_13: 0.5000 - lr: 0.0100\n",
      "Epoch 4/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.5132 - auc_13: 0.5000 - val_loss: 0.4940 - val_auc_13: 0.5000 - lr: 0.0100\n",
      "Epoch 5/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.5131 - auc_13: 0.5000 - val_loss: 0.4925 - val_auc_13: 0.5000 - lr: 0.0100\n",
      "Epoch 6/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.5127 - auc_13: 0.5000 - val_loss: 0.4936 - val_auc_13: 0.5000 - lr: 0.0100\n",
      "Epoch 7/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.5119 - auc_13: 0.5000 - val_loss: 0.4929 - val_auc_13: 0.5000 - lr: 0.0100\n",
      "Epoch 8/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.5109 - auc_13: 0.5000 - val_loss: 0.4941 - val_auc_13: 0.5000 - lr: 0.0100\n",
      "Epoch 9/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.5104 - auc_13: 0.5000 - val_loss: 0.4968 - val_auc_13: 0.5000 - lr: 0.0100\n",
      "Epoch 10/300\n",
      "208/213 [============================>.] - ETA: 0s - loss: 0.5098 - auc_13: 0.5000\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.006999999843537807.\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.5093 - auc_13: 0.5000 - val_loss: 0.4980 - val_auc_13: 0.5000 - lr: 0.0100\n",
      "Epoch 11/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.5074 - auc_13: 0.5000 - val_loss: 0.4966 - val_auc_13: 0.5000 - lr: 0.0070\n",
      "Epoch 12/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.5066 - auc_13: 0.5000 - val_loss: 0.5009 - val_auc_13: 0.5000 - lr: 0.0070\n",
      "Epoch 13/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.5057 - auc_13: 0.5000 - val_loss: 0.4995 - val_auc_13: 0.5000 - lr: 0.0070\n",
      "Epoch 14/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.5043 - auc_13: 0.5000 - val_loss: 0.5014 - val_auc_13: 0.5000 - lr: 0.0070\n",
      "Epoch 15/300\n",
      "211/213 [============================>.] - ETA: 0s - loss: 0.5033 - auc_13: 0.5000\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.004899999825283885.\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.5032 - auc_13: 0.5000 - val_loss: 0.5054 - val_auc_13: 0.5000 - lr: 0.0070\n",
      "Epoch 00015: early stopping\n",
      "================================== training 1 fin. predicting ... ==================================\n",
      "21256\n",
      "(21256, 1)\n",
      "Epoch 1/300\n",
      "213/213 [==============================] - 2s 3ms/step - loss: 0.5228 - auc_14: 0.5000 - val_loss: 0.5301 - val_auc_14: 0.5000 - lr: 0.0100\n",
      "Epoch 2/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.5096 - auc_14: 0.5000 - val_loss: 0.5210 - val_auc_14: 0.5000 - lr: 0.0100\n",
      "Epoch 3/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.5071 - auc_14: 0.5000 - val_loss: 0.5154 - val_auc_14: 0.5000 - lr: 0.0100\n",
      "Epoch 4/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.5068 - auc_14: 0.5000 - val_loss: 0.5158 - val_auc_14: 0.5000 - lr: 0.0100\n",
      "Epoch 5/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.5052 - auc_14: 0.5000 - val_loss: 0.5203 - val_auc_14: 0.5000 - lr: 0.0100\n",
      "Epoch 6/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.5053 - auc_14: 0.5000 - val_loss: 0.5165 - val_auc_14: 0.5000 - lr: 0.0100\n",
      "Epoch 7/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.5049 - auc_14: 0.5000 - val_loss: 0.5177 - val_auc_14: 0.5000 - lr: 0.0100\n",
      "Epoch 8/300\n",
      "205/213 [===========================>..] - ETA: 0s - loss: 0.5034 - auc_14: 0.5000\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.006999999843537807.\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.5041 - auc_14: 0.5000 - val_loss: 0.5160 - val_auc_14: 0.5000 - lr: 0.0100\n",
      "Epoch 9/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.5013 - auc_14: 0.5000 - val_loss: 0.5177 - val_auc_14: 0.5000 - lr: 0.0070\n",
      "Epoch 10/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.5016 - auc_14: 0.5000 - val_loss: 0.5175 - val_auc_14: 0.5000 - lr: 0.0070\n",
      "Epoch 11/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.4994 - auc_14: 0.5000 - val_loss: 0.5211 - val_auc_14: 0.5000 - lr: 0.0070\n",
      "Epoch 12/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.4995 - auc_14: 0.5000 - val_loss: 0.5265 - val_auc_14: 0.5000 - lr: 0.0070\n",
      "Epoch 13/300\n",
      "194/213 [==========================>...] - ETA: 0s - loss: 0.4982 - auc_14: 0.5000\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.004899999825283885.\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.4986 - auc_14: 0.5000 - val_loss: 0.5200 - val_auc_14: 0.5000 - lr: 0.0070\n",
      "Epoch 00013: early stopping\n",
      "================================== training 2 fin. predicting ... ==================================\n",
      "21256\n",
      "(21256, 1)\n",
      "Epoch 1/300\n",
      "213/213 [==============================] - 2s 4ms/step - loss: 0.5223 - auc_15: 0.5000 - val_loss: 0.5203 - val_auc_15: 0.5000 - lr: 0.0100\n",
      "Epoch 2/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.5106 - auc_15: 0.5000 - val_loss: 0.5173 - val_auc_15: 0.5000 - lr: 0.0100\n",
      "Epoch 3/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.5082 - auc_15: 0.5000 - val_loss: 0.5188 - val_auc_15: 0.5000 - lr: 0.0100\n",
      "Epoch 4/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.5067 - auc_15: 0.5000 - val_loss: 0.5191 - val_auc_15: 0.5000 - lr: 0.0100\n",
      "Epoch 5/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.5057 - auc_15: 0.5000 - val_loss: 0.5195 - val_auc_15: 0.5000 - lr: 0.0100\n",
      "Epoch 6/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.5047 - auc_15: 0.5000 - val_loss: 0.5169 - val_auc_15: 0.5000 - lr: 0.0100\n",
      "Epoch 7/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.5040 - auc_15: 0.5000 - val_loss: 0.5197 - val_auc_15: 0.5000 - lr: 0.0100\n",
      "Epoch 8/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.5041 - auc_15: 0.5000 - val_loss: 0.5241 - val_auc_15: 0.5000 - lr: 0.0100\n",
      "Epoch 9/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.5027 - auc_15: 0.5000 - val_loss: 0.5208 - val_auc_15: 0.5000 - lr: 0.0100\n",
      "Epoch 10/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.5022 - auc_15: 0.5000 - val_loss: 0.5232 - val_auc_15: 0.5000 - lr: 0.0100\n",
      "Epoch 11/300\n",
      "188/213 [=========================>....] - ETA: 0s - loss: 0.5015 - auc_15: 0.5000\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.006999999843537807.\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.5006 - auc_15: 0.5000 - val_loss: 0.5249 - val_auc_15: 0.5000 - lr: 0.0100\n",
      "Epoch 12/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.4976 - auc_15: 0.5000 - val_loss: 0.5220 - val_auc_15: 0.5000 - lr: 0.0070\n",
      "Epoch 13/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.4965 - auc_15: 0.5000 - val_loss: 0.5289 - val_auc_15: 0.5000 - lr: 0.0070\n",
      "Epoch 14/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.4963 - auc_15: 0.5000 - val_loss: 0.5257 - val_auc_15: 0.5000 - lr: 0.0070\n",
      "Epoch 15/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.4943 - auc_15: 0.5000 - val_loss: 0.5302 - val_auc_15: 0.5000 - lr: 0.0070\n",
      "Epoch 16/300\n",
      "194/213 [==========================>...] - ETA: 0s - loss: 0.4948 - auc_15: 0.5000\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.004899999825283885.\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.4943 - auc_15: 0.5000 - val_loss: 0.5311 - val_auc_15: 0.5000 - lr: 0.0070\n",
      "Epoch 00016: early stopping\n",
      "================================== training 3 fin. predicting ... ==================================\n",
      "21256\n",
      "(21256, 1)\n",
      "Epoch 1/300\n",
      "213/213 [==============================] - 2s 4ms/step - loss: 0.5377 - auc_16: 0.5000 - val_loss: 0.5128 - val_auc_16: 0.5000 - lr: 0.0100\n",
      "Epoch 2/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.5119 - auc_16: 0.5000 - val_loss: 0.5084 - val_auc_16: 0.5000 - lr: 0.0100\n",
      "Epoch 3/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.5102 - auc_16: 0.5000 - val_loss: 0.5091 - val_auc_16: 0.5000 - lr: 0.0100\n",
      "Epoch 4/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.5091 - auc_16: 0.5000 - val_loss: 0.5130 - val_auc_16: 0.5000 - lr: 0.0100\n",
      "Epoch 5/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.5086 - auc_16: 0.5000 - val_loss: 0.5090 - val_auc_16: 0.5000 - lr: 0.0100\n",
      "Epoch 6/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.5075 - auc_16: 0.5000 - val_loss: 0.5123 - val_auc_16: 0.5000 - lr: 0.0100\n",
      "Epoch 7/300\n",
      "194/213 [==========================>...] - ETA: 0s - loss: 0.5064 - auc_16: 0.5000\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.006999999843537807.\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.5070 - auc_16: 0.5000 - val_loss: 0.5109 - val_auc_16: 0.5000 - lr: 0.0100\n",
      "Epoch 8/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.5051 - auc_16: 0.5000 - val_loss: 0.5112 - val_auc_16: 0.5000 - lr: 0.0070\n",
      "Epoch 9/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.5036 - auc_16: 0.5000 - val_loss: 0.5125 - val_auc_16: 0.5000 - lr: 0.0070\n",
      "Epoch 10/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.5033 - auc_16: 0.5000 - val_loss: 0.5129 - val_auc_16: 0.5000 - lr: 0.0070\n",
      "Epoch 11/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.5022 - auc_16: 0.5000 - val_loss: 0.5105 - val_auc_16: 0.5000 - lr: 0.0070\n",
      "Epoch 12/300\n",
      "206/213 [============================>.] - ETA: 0s - loss: 0.5010 - auc_16: 0.5000\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.004899999825283885.\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.5013 - auc_16: 0.5000 - val_loss: 0.5184 - val_auc_16: 0.5000 - lr: 0.0070\n",
      "Epoch 00012: early stopping\n",
      "================================== training 4 fin. predicting ... ==================================\n",
      "21256\n",
      "(21256, 1)\n",
      "Epoch 1/300\n",
      "213/213 [==============================] - 2s 3ms/step - loss: 0.5285 - auc_17: 0.5000 - val_loss: 0.5246 - val_auc_17: 0.5000 - lr: 0.0100\n",
      "Epoch 2/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.5086 - auc_17: 0.5000 - val_loss: 0.5230 - val_auc_17: 0.5000 - lr: 0.0100\n",
      "Epoch 3/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.5067 - auc_17: 0.5000 - val_loss: 0.5227 - val_auc_17: 0.5000 - lr: 0.0100\n",
      "Epoch 4/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.5052 - auc_17: 0.5000 - val_loss: 0.5245 - val_auc_17: 0.5000 - lr: 0.0100\n",
      "Epoch 5/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.5050 - auc_17: 0.5000 - val_loss: 0.5229 - val_auc_17: 0.5000 - lr: 0.0100\n",
      "Epoch 6/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.5038 - auc_17: 0.5000 - val_loss: 0.5279 - val_auc_17: 0.5000 - lr: 0.0100\n",
      "Epoch 7/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.5034 - auc_17: 0.5000 - val_loss: 0.5283 - val_auc_17: 0.5000 - lr: 0.0100\n",
      "Epoch 8/300\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.5028 - auc_17: 0.5000\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.006999999843537807.\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.5028 - auc_17: 0.5000 - val_loss: 0.5268 - val_auc_17: 0.5000 - lr: 0.0100\n",
      "Epoch 9/300\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.5010 - auc_17: 0.5000 - val_loss: 0.5239 - val_auc_17: 0.5000 - lr: 0.0070\n",
      "Epoch 10/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.4998 - auc_17: 0.5000 - val_loss: 0.5256 - val_auc_17: 0.5000 - lr: 0.0070\n",
      "Epoch 11/300\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.4991 - auc_17: 0.5000 - val_loss: 0.5264 - val_auc_17: 0.5000 - lr: 0.0070\n",
      "Epoch 12/300\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 0.4993 - auc_17: 0.5000 - val_loss: 0.5263 - val_auc_17: 0.5000 - lr: 0.0070\n",
      "Epoch 13/300\n",
      "196/213 [==========================>...] - ETA: 0s - loss: 0.4991 - auc_17: 0.5000\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.004899999825283885.\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 0.4982 - auc_17: 0.5000 - val_loss: 0.5289 - val_auc_17: 0.5000 - lr: 0.0070\n",
      "Epoch 00013: early stopping\n",
      "================================== training 5 fin. predicting ... ==================================\n",
      "21256\n",
      "(21256, 1)\n",
      "\n",
      "Train Score : 0.70711\n",
      "Valid Score : 0.70711\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(Config.random_seed)\n",
    "tf.random.set_seed(Config.random_seed)\n",
    "\n",
    "kf = KFold(n_splits=Config.n_folds, shuffle=True, random_state=Config.random_seed)\n",
    "kf_encoding = KFold(n_splits=Config.n_folds, shuffle=True, random_state=Config.random_seed + 1)\n",
    "\n",
    "#split_series = df_train[split_col]\n",
    "#split_unique_series = df_train[split_col].unique()\n",
    "\n",
    "results = {}\n",
    "preds_test = np.zeros(len(X_test))\n",
    "stacking_preds_valid, stacking_idxes_valid = [], []\n",
    "\n",
    "for idx, (idx_train, idx_valid) in enumerate(kf.split(df_train)):\n",
    "    X_train = df_train.loc[idx_train][feature_list]\n",
    "    y_train = df_train.loc[idx_train][Config.target]\n",
    "    X_valid = df_train.loc[idx_valid][feature_list]\n",
    "    y_valid = df_train.loc[idx_valid][Config.target]\n",
    "\n",
    "    results[f'Fold{idx+1}'] = {}\n",
    "    preds_train = np.zeros(len(X_train))\n",
    "    preds_valid = np.zeros(len(X_valid))\n",
    "\n",
    "    X_len = len(X_train)\n",
    "    X_train = (X_train.values).astype(np.float32)\n",
    "    X_valid = (X_valid.values).astype(np.float32)\n",
    "\n",
    "    y_train = (y_train.values).astype(np.float32)\n",
    "    y_valid = (y_valid.values).astype(np.float32)\n",
    "\n",
    "    # training\n",
    "    model = setup_model()\n",
    "    callbacks = setup_callbacks()\n",
    "    hist = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=mlp_param['epochs'], batch_size=mlp_param['batch_size'], callbacks=callbacks, verbose=mlp_param['verbose'])\n",
    "    print(f'================================== training {idx + 1} fin. predicting ... ==================================')\n",
    "\n",
    "    # evaluation\n",
    "    # train_loss, train_rmse = model.evaluate(X_train, y_train, verbose=0)\n",
    "    # valid_loss, valid_rmse = model.evaluate(X_valid, y_valid, verbose=0)\n",
    "    # print(train_loss, train_rmse, valid_loss, valid_rmse)\n",
    "\n",
    "    print(len(model.predict(X_train)))\n",
    "    print(model.predict(X_train).shape)\n",
    "    preds_train = np.minimum(np.maximum(model.predict(X_train), 0), 1)\n",
    "    preds_valid = np.minimum(np.maximum(model.predict(X_valid), 0), 1)\n",
    "\n",
    "    auc_train = np.sqrt(roc_auc_score(y_train, preds_train))\n",
    "    auc_valid = np.sqrt(roc_auc_score(y_valid, preds_valid))\n",
    "\n",
    "    # save training data\n",
    "    results[f'Fold{idx + 1}']['datasets'] = [X_train, y_train, X_valid, y_valid]\n",
    "    results[f'Fold{idx + 1}']['index_train'] = df_train.loc[idx_train].index\n",
    "    results[f'Fold{idx + 1}']['index_valid'] = df_train.loc[idx_valid].index\n",
    "    results[f'Fold{idx + 1}']['model'] = model\n",
    "    results[f'Fold{idx + 1}']['hist'] = hist\n",
    "    results[f'Fold{idx + 1}']['preds_train'] = preds_train\n",
    "    results[f'Fold{idx + 1}']['preds_valid'] = preds_valid\n",
    "    results[f'Fold{idx + 1}']['score_train'] = auc_train\n",
    "    results[f'Fold{idx + 1}']['score_valid'] = auc_train\n",
    "\n",
    "    # predict for submission\n",
    "    preds_test +=  np.minimum(np.maximum(model.predict(X_test), 0), 1) / Config.n_folds\n",
    "\n",
    "    # stacking\n",
    "    stacking_preds_valid.append(results[f'Fold{idx + 1}']['preds_valid'])\n",
    "    stacking_idxes_valid.append(results[f'Fold{idx + 1}']['index_valid'])\n",
    "\n",
    "# output results\n",
    "train_score = valid_score = 0\n",
    "for i in range(Config.n_folds):\n",
    "    train_score += results[f'Fold{idx+1}']['score_train'] / Config.n_folds\n",
    "    valid_score += results[f'Fold{idx+1}']['score_valid'] / Config.n_folds\n",
    "\n",
    "print('')\n",
    "print(f'Train Score : {train_score:.5f}')\n",
    "print(f'Valid Score : {valid_score:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/interim/nb205_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/qr/28b40stn0vvfvznrz793zqgw0000gn/T/ipykernel_52218/31820721.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdf_test_stacking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'nb{Config.NB}'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpreds_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdf_train_stacking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'../data/interim/nb{Config.NB}_train.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mdf_test_stacking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'../data/interim/nb{Config.NB}_test.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.7/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3464\u001b[0m         )\n\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3466\u001b[0;31m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[1;32m   3467\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3468\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.7/lib/python3.9/site-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1103\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         )\n\u001b[0;32m-> 1105\u001b[0;31m         \u001b[0mcsv_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.7/lib/python3.9/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \"\"\"\n\u001b[1;32m    236\u001b[0m         \u001b[0;31m# apply compression and byte/text conversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         with get_handle(\n\u001b[0m\u001b[1;32m    238\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.7/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/interim/nb205_train.csv'"
     ]
    }
   ],
   "source": [
    "# stacking\n",
    "stacking_preds_valid = np.concatenate(stacking_preds_valid, axis=0)\n",
    "stacking_idxes_valid = np.concatenate(stacking_idxes_valid)\n",
    "stacking_order_valid = np.argsort(stacking_idxes_valid)\n",
    "stacking_preds_valid_sorted = stacking_preds_valid[stacking_order_valid]\n",
    "\n",
    "df_train_stacking = pd.DataFrame({Config.row_id: df_train[Config.row_id], f'nb{Config.NB}': stacking_preds_valid_sorted.reshape(-1)})\n",
    "df_test_stacking = pd.DataFrame({Config.row_id: df_test[Config.row_id], f'nb{Config.NB}': preds_test[:, 0]})\n",
    "\n",
    "df_train_stacking.to_csv(f'../data/interim/nb{Config.NB}_train.csv', index=False)\n",
    "df_test_stacking.to_csv(f'../data/interim/nb{Config.NB}_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(preds_test[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000001, 1.00000001, 1.00000001, ..., 1.00000001, 1.00000001,\n",
       "       1.00000001])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>failure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26570</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26571</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26572</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26573</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26574</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20770</th>\n",
       "      <td>47340</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20771</th>\n",
       "      <td>47341</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20772</th>\n",
       "      <td>47342</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20773</th>\n",
       "      <td>47343</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20774</th>\n",
       "      <td>47344</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20775 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  failure\n",
       "0      26570      0.0\n",
       "1      26571      0.0\n",
       "2      26572      0.0\n",
       "3      26573      0.0\n",
       "4      26574      0.0\n",
       "...      ...      ...\n",
       "20770  47340      0.0\n",
       "20771  47341      0.0\n",
       "20772  47342      0.0\n",
       "20773  47343      0.0\n",
       "20774  47344      0.0\n",
       "\n",
       "[20775 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 検証データの誤差の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(FOLDS):\n",
    "    train_loss = results[f'Fold{i + 1}']['hist'].history['loss']\n",
    "    val_loss = results[f'Fold{i + 1}']['hist'].history['val_loss']\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.rc('font', family='serif')\n",
    "    plt.plot(range(len(train_loss)), train_loss, color='blue', linewidth=1, label='train_loss')\n",
    "    plt.plot(range(len(val_loss)), val_loss, color='red', linewidth=1, label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.columns = [Config.row_id, Config.target]\n",
    "submission[Config.target] = preds_test\n",
    "submission.to_csv(f\"../data/submission/nb{Config.NB}.csv\", index=False, header=False)\n",
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "preds_index = []\n",
    "for i in range(FOLDS):\n",
    "    preds.append(results[f'Fold{i + 1}']['preds_valid'])\n",
    "    preds_index.append(results[f'Fold{i + 1}']['index_valid'])\n",
    "\n",
    "preds_index_tmp = np.concatenate(preds_index)\n",
    "preds_tmp = np.concatenate(preds, axis=0)\n",
    "\n",
    "order = np.argsort(preds_index_tmp)\n",
    "preds_sorted = preds_tmp[order]\n",
    "\n",
    "print(f'valid RMSE : {np.sqrt(mean_squared_error(df_train[TARGET], preds_sorted))}')\n",
    "\n",
    "df_train_eval = df_train\n",
    "df_train_eval['preds'] = preds_sorted\n",
    "\n",
    "df_train_eval = df_train_eval[['id', 'year', 'month', 'day', 'Country', 'City', 'lat', 'lon', 'pm25_mid', 'preds']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_tmp = pd.read_pickle(\"../data/processed/nb101_train.pkl\", compression='zip')\n",
    "df_test_tmp = pd.read_pickle(\"../data/processed/nb101_test.pkl\", compression='zip')\n",
    "df_train_tmp['datetime'] = df_train_tmp['year'] * 10000 + df_train_tmp['month'] * 100 + df_train_tmp['day']\n",
    "df_train_tmp['datetime'] = df_train_tmp['datetime'].astype(str)\n",
    "df_train_tmp['datetime'] = pd.to_datetime(df_train_tmp['datetime'])\n",
    "\n",
    "df_train_eval['datetime'] = df_train_tmp['datetime']\n",
    "df_train_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in df_train_eval['City'].unique():\n",
    "    y_true = df_train_eval[df_train_eval['City'] == city]['pm25_mid']\n",
    "    y_pred = df_train_eval[df_train_eval['City'] == city]['preds']\n",
    "    country = df_train_eval[df_train_eval['City'] == city]['Country'].unique()[0]\n",
    "\n",
    "    city_rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "    if city_rmse > 35:\n",
    "        print(f'{city_rmse} in {city} {country}')\n",
    "\n",
    "    if city_rmse > 45:\n",
    "        plt.figure(figsize=(40, 10))\n",
    "        plt.title(f'PM2.5 prediction RMSE:{city_rmse} in {city}')\n",
    "        plt.plot(df_train_eval[df_train_eval['City'] == city]['datetime'], y_true, label='y_true')\n",
    "        plt.plot(df_train_eval[df_train_eval['City'] == city]['datetime'], y_pred, label='y_pred')\n",
    "        plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 検証メモ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_tmp = pd.read_pickle(\"../data/processed/nb101_train.pkl\", compression='zip')\n",
    "df_test_tmp = pd.read_pickle(\"../data/processed/nb101_test.pkl\", compression='zip')\n",
    "df_train_tmp['datetime'] = df_train_tmp['year'] * 10000 + df_train_tmp['month'] * 100 + df_train_tmp['day']\n",
    "df_train_tmp['datetime'] = df_train_tmp['datetime'].astype(str)\n",
    "df_train_tmp['datetime'] = pd.to_datetime(df_train_tmp['datetime'])\n",
    "df_train_tmp['datetime_month'] = df_train_tmp.datetime.dt.month\n",
    "df_train_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_eval[df_train_eval['City'] == 'Denver']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_eval['diff'] = df_train_eval['pm25_mid'] - df_train_eval['preds']\n",
    "df_train_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_eval[df_train_eval['diff'] > 150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_eval[df_train_eval['diff'] < -100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp1 = df_tmp\n",
    "df_tmp1['Country'] = df_tmp['Country'].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp1[df_tmp1['Country'] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = df_train[is_valid][FEATURES]\n",
    "data_tmp = pd.DataFrame({c: X_train[c], 'target': y_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mean = data_tmp.groupby(c)['target'].mean()\n",
    "X_valid.loc[:, c] = X_valid[c].map(target_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = df_train[is_valid][FEATURES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tmp = pd.DataFrame({c: X_train[c], 'target': y_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dc8a78a13283e3ba74119858067a74c2c7a55702e09c935fdd8fe4b244251524"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('3.9.7': pyenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
